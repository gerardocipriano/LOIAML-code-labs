{"cells":[{"cell_type":"markdown","metadata":{"id":"lSIbDbwGFhYV"},"source":["# Fondamenti di elaborazione immagini"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lQlwyiStFt8J"},"source":["## Effettuiamo l'import delle librerie utilizzate nell'esercitazione."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1571,"status":"ok","timestamp":1678897543156,"user":{"displayName":"Federico Baire","userId":"14830023662958388668"},"user_tz":-60},"id":"gN3lci9bFhYY"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib as mapli\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import ImageGrid\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"u-6ozbN3FhYZ"},"source":["Di seguito i riferimenti alle pagine di documentazione, sempre utiliti:\n","\n","* Rif: [numpy](https://numpy.org/doc/stable/)\n","* Rif: [opencv](https://docs.opencv.org/)\n","* Rif: [matplotlib](https://matplotlib.org/stable/index.html)"]},{"cell_type":"markdown","metadata":{"id":"EMCUsHjPFhYZ"},"source":["Aggiungiamo alcune funzioni di utilita' per la scrittura del codice."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1678897543747,"user":{"displayName":"Federico Baire","userId":"14830023662958388668"},"user_tz":-60},"id":"ractI2PQFhYZ"},"outputs":[],"source":["def rgb(image : np.array) -> np.array:\n","    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","def gray(image : np.array) -> np.array:\n","    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","def grid(images : list[np.array], rows : int, cols : int, size : int, colors : list[str] = None) -> None:\n","    fig = plt.figure(figsize=(size,size))\n","    grid = ImageGrid(fig, 111, nrows_ncols=(rows, cols), axes_pad=0.1)\n","\n","    if colors is not None:\n","        counter = 0\n","        for ax, im in zip(grid, images):\n","            ax.imshow(im, cmap=colors[counter])\n","            counter = (counter + 1) % len(colors)\n","        plt.show()\n","    else:\n","        for ax, im in zip(grid, images):\n","            ax.imshow(im)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"grYm5vZyFhYa"},"source":["## _Sappiamo che le immagini sono matrici di numeri e, in quanto tali, operazioni aritmentiche elemento per elemento sono permesse e significative._"]},{"cell_type":"markdown","metadata":{"id":"oXJkY4M9F7-g"},"source":["Le operazioni applicabili sono molte e combinabili fra loro percio', di seguito, mostreremo solo alcuni esempi."]},{"cell_type":"markdown","metadata":{"id":"AATZvVF5FhYa"},"source":["Un esempio di applicazione della sottrazione lo possiamo trovare con la ricerca delle differenze. Quando, ad esempio, si vogliono realizzare architetture in cui si localizzano oggetti in movimento, diventa importante capire come da una scena all'altra avvengano i cambiamenti. Con il valore assoluto di una differenza pixel a pixel, possiamo ottenere un risultato significativo. Usiamo il metodo _absdiff_ di _opencv_.\n","\n","* Rif: [absdiff](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#ga6fef31bc8c4071cbc114a758a2b79c14)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ImXTAAwFhYa","outputId":"9d6b6c6f-8118-4cce-ed12-956236943c87"},"outputs":[],"source":["img_a = cv2.imread('./imgs/differences/a.png')\n","img_b = cv2.imread('./imgs/differences/b.png')\n","\n","grid([rgb(img_a), rgb(img_b)], 1, 2, 30)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Eseguiamone quindi la differenza."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diff = cv2.absdiff(img_a, img_b)\n","\n","grid([rgb(img_a), rgb(img_b), rgb(diff)], 1, 3, 30)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HB2QBUWHFhYb"},"source":["Con una semplice sottrazione, saturata ai valori del formato uint8 (0-255), non avremmo ottenuto un risultato altrettanto apprezzabile."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvxsYYnfFhYc","outputId":"174d22e8-8dd0-4564-b856-24d178b16052"},"outputs":[],"source":["saturated_diff = cv2.subtract(img_a, img_b)\n","\n","grid([rgb(img_a), rgb(img_b), rgb(diff), rgb(saturated_diff)], 1, 4, 30)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0wa9xjbIFhYc"},"source":["## _Un ulteriore esempio, prassi standard della fase di preprocessamento delle immagini in un dataset e' la sottrazione con la media e la divisione per la deviazione standard._"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Questo tipo di processamento, chiamato **normalizzazione** ha lo scopo di ridimensionare la scala e la varianza dei dati."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOpI-OghFhYc","outputId":"bde18caf-6dbd-4d2a-d320-2386dc8ee9e6"},"outputs":[],"source":["apple_1 = cv2.imread('./imgs/apples/apple_1.png')\n","apple_2 = cv2.imread('./imgs/apples/apple_2.png')\n","apple_3 = cv2.imread('./imgs/apples/apple_3.png')\n","\n","apples = [apple_1, apple_2, apple_3]\n","\n","apples_mean = np.mean(apples, axis=(0, 1, 2))   # media rispettivamente per r, g, b\n","apples_std = np.std(apples, axis=(0, 1, 2))     # deviazione standard rispettivamente per r, g, b\n","\n","print(f'mean: {apples_mean}, std: {apples_std}')"]},{"cell_type":"markdown","metadata":{},"source":["Ad aiutarci, in questo caso, sono stati i metodi _numpy_ chiamati _mean_ e _std_ che rispettivamente calcolano media e deviazione standard lungo le dimensioni specificate.\n","\n","* Rif: [mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\n","* Rif: [std](https://numpy.org/doc/stable/reference/generated/numpy.std.html)\n","\n","Procediamo quindi ad eseguire la normalizzazione."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWuF1qYRFhYd","outputId":"b7f42c26-0c1c-4f22-8a2b-57c681175436"},"outputs":[],"source":["norm_apple_1 = (apple_1 - apples_mean) / apples_std\n","norm_apple_2 = (apple_2 - apples_mean) / apples_std\n","norm_apple_3 = (apple_3 - apples_mean) / apples_std\n","\n","apples = [rgb(apple_1), rgb(apple_2), rgb(apple_3)]\n","\n","norm_apple_1 = (255 * norm_apple_1).astype(np.uint8)\n","norm_apple_2 = (255 * norm_apple_2).astype(np.uint8)\n","norm_apple_3 = (255 * norm_apple_3).astype(np.uint8)\n","\n","norm_apples = [rgb(norm_apple_1), rgb(norm_apple_2), rgb(norm_apple_3)]\n","\n","grid(apples, 1, 3, 30)\n","grid(norm_apples, 1, 3, 30)"]},{"cell_type":"markdown","metadata":{"id":"Wv2Q5bnVFhYd"},"source":["La normalizzazione, per il deep learning, risulta essere una tecnica di preprocessamento fondamentale al fine di ottenere risultati sotto diversi aspetti:\n","\n","* Si riduce la possibilita' che i dati assumano valori \"troppo alti\" o \"troppo bassi\" che, rispettivamente, potrebbero saturare o diventare ininfluenti al fine di un corretto addestramento.\n","* Si agevola la convergenza verso un risultato nel problema di ottimizzazione rendendo i valori indipenti dalla loro scala e centrandone la distribuzione attorno allo zero.\n","* Si riduce il peso computazionale dell'addestramento, riducendo di fatto il range che i valori possono assumere durante quest'ultimo."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"corso_ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"55b72797edd57f58696da9ac5b5536b8813fefe094e8c8c02c797501a07dae2a"}}},"nbformat":4,"nbformat_minor":0}
