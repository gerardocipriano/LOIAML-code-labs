{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fondamenti di Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effettuiamo l'import delle librerie utilizzate nell'esercitazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito i riferimenti alle pagine di documentazione, sempre utiliti:\n",
    "\n",
    "* Rif: [numpy](https://numpy.org/doc/stable/)\n",
    "* Rif: [pytorch](https://pytorch.org/docs/stable/index.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungiamo alcune funzioni di utilita' per semplificare la scrittura del codice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(t : torch.Tensor):\n",
    "    print(f'\\n*****')\n",
    "    print(f'Valore:\\n{t}\\n')\n",
    "    print(f'Tipo pytohn\\t: {type(t)}')\n",
    "    print(f'Tipo\\t\\t: {t.dtype}')\n",
    "    print(f'Dimensioni\\t: {t.ndim}')\n",
    "    print(f'Forma\\t\\t: {t.shape}')\n",
    "    print(f'Dispositivo\\t: {t.device}')\n",
    "    print(f'*****\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Imparato i concetti principali legati ai tensori, e' possibile vederli in uso eseguendovi operazioni e analizzandoli._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tensori sono matrici **n-dimensionali**; questo significa che l'algebra che regola le operazioni fra matrici resta valida: somma/sottrazione/divisione/moltiplicazione per scalari, per altre matrici, prodotto matriciale...Vediamone degli esempi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un tensore di prova e manipoliamolo con uno scalare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "t_scalar = torch.tensor(5)\n",
    "\n",
    "info(t_tensor)\n",
    "info(t_scalar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Somma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_result = t_tensor + t_scalar\n",
    "info(t_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sottrazione_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_result = t_tensor - t_scalar\n",
    "info(t_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Moltiplicazione_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_result = t_tensor * t_scalar\n",
    "info(t_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Divisione_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_result = t_tensor / t_scalar\n",
    "info(t_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo come, in questo caso, la divisione ha prodotto valori con la virgola e, di conseguenza, il tipo di dato del risultato sia stato convertito per ospitare i valori decimali."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Le operazioni viste, hanno anche una loro controparte \"pytorch\". Ne esisto molte altre che, nella matrice e fra matrici, permettono di realizzare analisi di medie, varianze, valori assoluti..._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Valore assoluto_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rif: [abs](https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor = torch.tensor([[-1, -2], [-3, -4]])\n",
    "info(t_tensor)\n",
    "\n",
    "t_tensor = t_tensor.abs()\n",
    "info (t_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Media e varianza_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Media e varianza, come anche avviene per _numpy_, sono eseguite lungo la dimensione indicata. Non indicando alcuna dimensione (_None_) si mediano tutti gli elementi. La dimensione 0 media le righe fra loro, 1 le colonne.\n",
    "\n",
    "* Rif: [mean](https://pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean)\n",
    "* Rif: [std](https://pytorch.org/docs/stable/generated/torch.Tensor.std.html#torch.Tensor.std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per eseguire alcune operazioni il tipo richiesto e' a valori reali percio' risulta necessario cambiare il tipo al tensore se non coerente. Per farlo si indica il nuovo tipo al metodo _type_ applicato al tensore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_float = t_tensor.type(torch.float32)\n",
    "\n",
    "info(t_tensor_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Media lungo dimensione None: {t_tensor_float.mean()}')\n",
    "print(f'Media lungo dimensione 0: {t_tensor_float.mean(dim=0)}')\n",
    "print(f'Media lungo dimensione 1: {t_tensor_float.mean(dim=1)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo stesso valore per la deviazione standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Media lungo dimensione None: {t_tensor_float.std()}')\n",
    "print(f'Media lungo dimensione 0: {t_tensor_float.std(dim=0)}')\n",
    "print(f'Media lungo dimensione 1: {t_tensor_float.std(dim=1)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al seguente riferimento si possono consultare molte altre operazioni.\n",
    "\n",
    "* Rif: [PyTorch tensor](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Il prodotto fra tensori, non limitato al prodotto elemento per elemento, ha in pytorch un ruolo fondamentale._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo vale sopratutto nell'ambito del ML/DL in cui gran parte delle operazioni puo' infine essere ricondotta a prodotti fra tensori. In _pytorch_ esistono per questo svariati metodi per eseguire moltiplicazioni fra tensori; ognuno con il scopo e ambito di utilizzo. Di seguito alcuni riferimenti:\n",
    "\n",
    "* Rif: [mm](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch+mm#torch.mm)\n",
    "* Rif: [mv](https://pytorch.org/docs/stable/generated/torch.mv.html?highlight=torch+mv#torch.mv)\n",
    "* Rif: [bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html?highlight=torch+bmm#torch.bmm)\n",
    "* Rif: [matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=torch+matmul#torch.matmul)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo quindi tensori di dimensione superiore a 0, non scalari."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _La moltiplicazione fra due tensori 1-dimensionali esegue il prodotto di elementi corrispondenti e somma i risultati fra loro. Si ottiene quindi uno scalare._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_a = torch.tensor([1, 2, 3])\n",
    "t_tensor_b = torch.tensor([4, 5, 6])\n",
    "\n",
    "info(t_tensor_a)\n",
    "info(t_tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_ab = torch.matmul(t_tensor_a, t_tensor_b)\n",
    "info(t_tensor_ab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Il prodotto fra tensori 2-dimensionali e' a tutti gli effetti un prodotto fra matrici; per questo, le dimensioni interne delle matrici devono coincidere._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_c = torch.tensor([[1, 2],[3, 4],[5, 6]])\n",
    "t_tensor_d = torch.tensor([[1, 1, 1, 1],[2, 2, 2, 2]])\n",
    "\n",
    "info(t_tensor_c)\n",
    "info(t_tensor_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il tensore finale sara' sempre una matrice, 2 dimensioni, ma avra un numero di elementi pari al numero degli elementi \"esterni\" delle matrici di partenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_cd = torch.matmul(t_tensor_c, t_tensor_d)\n",
    "info(t_tensor_cd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _E' altrettanto possibile moltiplicare un tensore 2-dimensionale con un tensore 1-dimensionale e viceversa. (matrice * vettore, vettore * matrice)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info(t_tensor_a)\n",
    "info(t_tensor_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_ac = torch.matmul(t_tensor_a, t_tensor_c)\n",
    "info(t_tensor_ac)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il prodotto vettore-matrice si riconduce ad un prodotto fra matrici fingendo che il vettore iniziale abbia una dimensione in piu'. Valgono le regole del prodotto fra matrici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_e = torch.tensor([9, 9])\n",
    "\n",
    "info(t_tensor_c)\n",
    "info(t_tensor_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tensor_ce = torch.matmul(t_tensor_c, t_tensor_e)\n",
    "info(t_tensor_ce)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Superando le 2 dimensioni, il prodotto e' altrettanto possibile; ovviamente con i suoi vincoli e le sue regole._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo ad esempio di lavorare con delle immagini colore (rgb). Queste possono essere rappresentate come delle matrici di numeri dove pero' ogni elemento e' una raccolta di 3 valori. Proviamo a creare due immagini colore casuali di dimensione 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_image_sample_a = torch.randint(low=0, high=256, size=(1,3,3,3))\n",
    "info(t_image_sample_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_image_sample_b = torch.randint(low=0, high=256, size=(1,3,3,3))\n",
    "info(t_image_sample_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info(torch.matmul(t_image_sample_a, t_image_sample_b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragionare su tensori di dimensione superiore a 2 diventa sempre piu' complesso, complice il fatto di faticare a visualizzare le operazioni. L'esempio, molto semplice, e' difatti solamente la punta di un iceber molto piu' grande."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corso_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55b72797edd57f58696da9ac5b5536b8813fefe094e8c8c02c797501a07dae2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
